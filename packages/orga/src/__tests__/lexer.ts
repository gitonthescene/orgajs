import Lexer from '../lexer'

describe('Lexer', () => {
  const lexer = new Lexer({ timezone: `Pacific/Auckland` })

  it('knows table row', () => {
    expect(lexer.tokenize('| batman | superman | wonder woman |')).toMatchSnapshot()
  })

  it('knows blank', () => {
    //expect(lexer.tokenize('')).toMatchSnapshot()
    expect(lexer.tokenize(' ')).toMatchSnapshot()
    expect(lexer.tokenize('    ')).toMatchSnapshot()
    expect(lexer.tokenize('\t')).toMatchSnapshot()
    expect(lexer.tokenize(' \t')).toMatchSnapshot()
    expect(lexer.tokenize('\t ')).toMatchSnapshot()
    expect(lexer.tokenize(' \t  ')).toMatchSnapshot()
  })

  it('knows these are not blanks', () => {
    expect(lexer.tokenize(' a ')).toMatchSnapshot()
  })

  it('knows headlines', () => {
    expect(lexer.tokenize('** a headline')).toMatchSnapshot()
    expect(lexer.tokenize('** _headline_')).toMatchSnapshot()
    expect(lexer.tokenize('**   a headline')).toMatchSnapshot()
    expect(lexer.tokenize('***** a headline')).toMatchSnapshot()
    expect(lexer.tokenize('* a ðŸ˜€line')).toMatchSnapshot()
    expect(lexer.tokenize('* TODO [#A] a headline     :tag1:tag2:')).toMatchSnapshot()
  })

  it('knows these are not headlines', () => {
    expect(lexer.tokenize('*not a headline')).toMatchSnapshot()
    expect(lexer.tokenize(' * not a headline')).toMatchSnapshot()
    expect(lexer.tokenize('*_* not a headline')).toMatchSnapshot()
    expect(lexer.tokenize('not a headline')).toMatchSnapshot()
  })

  it('knows keywords', () => {
    expect(lexer.tokenize('#+KEY: Value')).toMatchSnapshot()
    expect(lexer.tokenize('#+KEY: Another Value')).toMatchSnapshot()
    expect(lexer.tokenize('#+KEY: value : Value')).toMatchSnapshot()
  })

  it('knows these are not keywords', () => {
    expect(lexer.tokenize('#+KEY : Value')).toMatchSnapshot()
    expect(lexer.tokenize('#+KE Y: Value')).toMatchSnapshot()
  })

  it('knows plannings', () => {
    expect(lexer.tokenize('DEADLINE: <2018-01-01 Mon>')).toMatchSnapshot()
    expect(lexer.tokenize('  DEADLINE: <2018-01-01 Mon>')).toMatchSnapshot()
    expect(lexer.tokenize(' \tDEADLINE: <2018-01-01 Mon>')).toMatchSnapshot()
    expect(lexer.tokenize(' \t DEADLINE: <2018-01-01 Mon>')).toMatchSnapshot()
  })

  it('knows these are not plannings', () => {
    expect(lexer.tokenize('dEADLINE: <2018-01-01 Mon>')).toMatchSnapshot()
  })

  it('knows block begins', () => {
    expect(lexer.tokenize('#+BEGIN_SRC swift')).toMatchSnapshot()
    expect(lexer.tokenize(' #+BEGIN_SRC swift')).toMatchSnapshot()
    expect(lexer.tokenize('#+begin_src swift')).toMatchSnapshot()
    expect(lexer.tokenize('#+begin_example')).toMatchSnapshot()
    expect(lexer.tokenize('#+begin_exðŸ˜€mple')).toMatchSnapshot()
    expect(lexer.tokenize('#+begin_src swift :tangle code.swift')).toMatchSnapshot()
  })

  it('knows these are not block begins', () => {
    expect(lexer.tokenize('#+begiðŸ˜€n_src swift')).toMatchSnapshot()
  })

  it('knows block ends', () => {
    expect(lexer.tokenize('#+END_SRC')).toMatchSnapshot()
    expect(lexer.tokenize('  #+END_SRC')).toMatchSnapshot()
    expect(lexer.tokenize('#+end_src')).toMatchSnapshot()
    expect(lexer.tokenize('#+end_SRC')).toMatchSnapshot()
    expect(lexer.tokenize('#+end_SðŸ˜€RC')).toMatchSnapshot()
  })

  it('knows these are not block ends', () => {
    expect(lexer.tokenize('#+end_SRC ')).toMatchSnapshot()
    expect(lexer.tokenize('#+end_src param')).toMatchSnapshot()
  })

  it('knows horizontal rules', () => {
    expect(lexer.tokenize('-----')).toMatchSnapshot()
    expect(lexer.tokenize('------')).toMatchSnapshot()
    expect(lexer.tokenize('--------')).toMatchSnapshot()
    expect(lexer.tokenize('  -----')).toMatchSnapshot()
    expect(lexer.tokenize('-----   ')).toMatchSnapshot()
    expect(lexer.tokenize('  -----   ')).toMatchSnapshot()
    expect(lexer.tokenize('  -----  \t ')).toMatchSnapshot()
  })

  it('knows these are not horizontal rules', () => {
    expect(lexer.tokenize('----')).toMatchSnapshot()
    expect(lexer.tokenize('- ----')).toMatchSnapshot()
    expect(lexer.tokenize('-----a')).toMatchSnapshot()
    expect(lexer.tokenize('_-----')).toMatchSnapshot()
    expect(lexer.tokenize('-----    a')).toMatchSnapshot()
  })

  it('knows comments', () => {
    expect(lexer.tokenize('# a comment')).toMatchSnapshot()
    expect(lexer.tokenize('# ')).toMatchSnapshot()
    expect(lexer.tokenize('# a commentðŸ˜¯')).toMatchSnapshot()
    expect(lexer.tokenize(' # a comment')).toMatchSnapshot()
    expect(lexer.tokenize('  \t  # a comment')).toMatchSnapshot()
    expect(lexer.tokenize('#   a comment')).toMatchSnapshot()
    expect(lexer.tokenize('#    \t a comment')).toMatchSnapshot()
  })

  it('knows these are not comments', () => {
    expect(lexer.tokenize('#not a comment')).toMatchSnapshot()
    expect(lexer.tokenize('  #not a comment')).toMatchSnapshot()
  })

  it('knows list items', () => {
    // unordered
    expect(lexer.tokenize('- buy milk')).toMatchSnapshot()
    expect(lexer.tokenize('+ buy milk')).toMatchSnapshot()
    // ordered
    expect(lexer.tokenize('1. buy milk')).toMatchSnapshot()
    expect(lexer.tokenize('12. buy milk')).toMatchSnapshot()
    expect(lexer.tokenize('123) buy milk')).toMatchSnapshot()
    // checkbox
    expect(lexer.tokenize('- [x] buy milk checked')).toMatchSnapshot()
    expect(lexer.tokenize('- [X] buy milk checked')).toMatchSnapshot()
    expect(lexer.tokenize('- [-] buy milk checked')).toMatchSnapshot()
    expect(lexer.tokenize('- [ ] buy milk unchecked')).toMatchSnapshot()
    // indent
    expect(lexer.tokenize('  - buy milk')).toMatchSnapshot()
    // tag
    expect(lexer.tokenize('- item1 :: description here')).toMatchSnapshot()
    expect(lexer.tokenize('- item2\n :: description here')).toMatchSnapshot()
    expect(lexer.tokenize('- [x] item3 :: description here')).toMatchSnapshot()
  })

  it('knows these are not list items', () => {
    expect(lexer.tokenize('-not item')).toMatchSnapshot()
    expect(lexer.tokenize('1.not item')).toMatchSnapshot()
    expect(lexer.tokenize('8)not item')).toMatchSnapshot()
    expect(lexer.tokenize('8a) not item')).toMatchSnapshot()
  })

  it('knows footnotes', () => {
    expect(lexer.tokenize('[fn:1] a footnote')).toMatchSnapshot()
    expect(lexer.tokenize('[fn:word] a footnote')).toMatchSnapshot()
    expect(lexer.tokenize('[fn:word_] a footnote')).toMatchSnapshot()
    expect(lexer.tokenize('[fn:wor1d_] a footnote')).toMatchSnapshot()
  })

  it('knows these are not footnotes', () => {
    expect(lexer.tokenize('[fn:1]: not a footnote')).toMatchSnapshot()
    expect(lexer.tokenize(' [fn:1] not a footnote')).toMatchSnapshot()
    expect(lexer.tokenize('[[fn:1] not a footnote')).toMatchSnapshot()
    expect(lexer.tokenize('\t[fn:1] not a footnote')).toMatchSnapshot()
  })

  it('knows table separators', () => {
    expect(lexer.tokenize('|----+---+----|')).toMatchSnapshot()
    expect(lexer.tokenize('|--=-+---+----|')).toMatchSnapshot()
    expect(lexer.tokenize('  |----+---+----|')).toMatchSnapshot()
    expect(lexer.tokenize('|----+---+----')).toMatchSnapshot()
    expect(lexer.tokenize('|---')).toMatchSnapshot()
    expect(lexer.tokenize('|-')).toMatchSnapshot()
  })

  it('knows these are not table separators', () => {
    expect(lexer.tokenize('----+---+----|')).toMatchSnapshot()
  })

  it('knows table rows', () => {
    expect(lexer.tokenize('| hello | world | y\'all |')).toMatchSnapshot()
    expect(lexer.tokenize('   | hello | world | y\'all |')).toMatchSnapshot()
    expect(lexer.tokenize('|    hello |  world   |y\'all |')).toMatchSnapshot()
    // with empty cell
    expect(lexer.tokenize('||  world   | |')).toMatchSnapshot()
  })

  it('knows these are not table rows', () => {
    expect(lexer.tokenize(' hello | world | y\'all |')).toMatchSnapshot()
    expect(lexer.tokenize('|+')).toMatchSnapshot()
  })

  it('knows drawer begins', () => {
    expect(lexer.tokenize(':PROPERTIES:')).toMatchSnapshot()
    expect(lexer.tokenize('  :properties:')).toMatchSnapshot()
    expect(lexer.tokenize('  :properties:  ')).toMatchSnapshot()
    expect(lexer.tokenize('  :prop_erties:  ')).toMatchSnapshot()
  })

  it('knows these are not drawer begins', () => {
    expect(lexer.tokenize('PROPERTIES:')).toMatchSnapshot()
    expect(lexer.tokenize(':PROPERTIES')).toMatchSnapshot()
    expect(lexer.tokenize(':PR OPERTIES:')).toMatchSnapshot()
  })

  it('knows drawer ends', () => {
    expect(lexer.tokenize(':END:')).toMatchSnapshot()
    expect(lexer.tokenize('  :end:')).toMatchSnapshot()
    expect(lexer.tokenize('  :end:  ')).toMatchSnapshot()
    expect(lexer.tokenize('  :end:  ')).toMatchSnapshot()
  })

  it('knows these are not drawer ends', () => {
    expect(lexer.tokenize('END:')).toMatchSnapshot()
    expect(lexer.tokenize(':END')).toMatchSnapshot()
    expect(lexer.tokenize(':ENDed')).toMatchSnapshot()
  })

  it('knows these are timestamps', () => {
    expect(lexer.tokenize('<2019-08-19 Mon>')).toMatchSnapshot()
    expect(lexer.tokenize('<2019-08-19 Mon 13:20>')).toMatchSnapshot()
    expect(lexer.tokenize('<2019-08-19 Mon 13:20-14:00>')).toMatchSnapshot()
    expect(lexer.tokenize('<2019-08-19 Mon>--<2019-08-20 Tue>')).toMatchSnapshot()
  })

})
